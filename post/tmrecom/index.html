<!DOCTYPE html>
<html lang="ja">
    <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>
				トピックモデルを用いた類似記事のレコメンド機能の実装 &middot; いのうの作業ログ
		</title>

		
  		<link rel="stylesheet" href="https://seiichiinoue.github.io/css/style.css">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
		
		<link rel="icon" type="image/png" sizes="32x32" href="https://seiichiinoue.github.io/img/favicon.png">
        
		<link rel="apple-touch-icon" sizes="180x180" href="https://seiichiinoue.github.io/img/favicon.png">
        
		
		<link href="" rel="alternate" type="application/rss+xml" title="いのうの作業ログ" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="https://seiichiinoue.github.io/">
					<h2 class="nav-title">いのうの作業ログ</h2>
				</a>
				<ul>
    <li><a href="https://seiichiinoue.github.io/about">About</a></li>
    <li><a href="https://seiichiinoue.github.io/">Posts</a></li>
</ul>

			</div>
		</nav>

        

<main>
	<div class="post">
		<div class="post-info">
        <br>
        <time datetime="2020-01-31 20:20:13 &#43;0900 JST">January 31, 2020</time>
</div>

		<h1 class="post-title">トピックモデルを用いた類似記事のレコメンド機能の実装</h1>
<div class="post-line"></div>

		

		

<p>久々の更新となります．トピックモデルによる文書分類を用いてこのブログのレコメンド機能を実装したので，まとめていきたいと思います．</p>

<p>大まかな流れとしては，ブログの全文書を用いたモデリング，モデルのAPI化，APIのデプロイ，DOMをいじって結果を表示，という感じになっています．かなり単純なのですが，自分にとっては初めてのことが多かったので（特にwebに関することで）備忘録的な形でも残せたらなと思って書いていきます．</p>

<p>まず，トピックモデルは，様々な離散データに隠れた潜在的なトピックを推定するモデルです．ここでいうトピックとは，自然言語処理の文脈では，話題や分野に対応すると考えられ，購買データなどでは，消費者の嗜好などに対応すると考えることができます．</p>

<p>また，ナイーブベイズ分類器などとは異なり，完全に教師なし（人手を介さない）でデータから自動的に学習するモデルとなっています．</p>

<p>今回は，そのトピックモデルのひとつであるLDA; Latent Dirichlet Allocationを用いてこのブログのモデリングを行いました．</p>

<h2 id="lda-latent-dirichlet-allocation">LDA; Latent Dirichlet Allocation</h2>

<p>LDAはざっくりと以下のようなモデルになります．</p>

<ul>
<li>文書の背景には「トピックの適合率」が存在している</li>
<li>文書中の各単語の背景には，一つのトピックが存在している</li>
<li>各単語の背景にあるトピックは，その単語が属している文書の「トピックの混合率」から生成される</li>
</ul>

<p>つまり，LDAによる文書群のモデル化ができれば，任意の単語に対して，単語の背景にあるトピックが推定可能なので，同じトピックに属する単語を探すことでその単語と同じような意味をもつ単語でクラスタリングすることができます．また，文書についても同様に，各文書に現れる背景トピックの数を見れば，文書のクラスタリングも可能になります．</p>

<h2 id="ldaの生成モデル">LDAの生成モデル</h2>

<p>LDAでは，まず，文書$\mathbf{w}$をトピックの混合比(=潜在トピック分布)で表現します．各文書のトピックは多項分布に従うので，そのトピックの混合比は，多項分布の共役事前分布のディリクレ分布から生成されると考えます．</p>

<p>次に，文書においてトピックが生成されたら，そのトピックにおける単語の生起確率分布を考えます．やはりこちらも同様に，単語の生起確率もディリクレ分布に従います．</p>

<p>そして，各単語は，単語が属する文書がもつトピックにおける単語の生起確率分布に従って生成されます．</p>

<p>以上のLDAにおける単語の生成モデルの流れをまとめると，以下のようになります．</p>

<ul>
<li>トピックの混合比 $\theta \sim Dir(\alpha)$ を生成</li>
<li>For n = 1 &hellip; N,

<ul>
<li>トピック $z_n \sim Mult(\theta)$ を選択</li>
<li>単語 $w_n \sim p(w|z)$ を生成</li>
</ul></li>
</ul>

<p>これが生成モデルですが，もちろん現実には文書，つまり単語のカウント数というデータがあるだけで，この生成過程はわかっていません．それをデータから求めます．</p>

<h2 id="ldaの解法">LDAの解法</h2>

<p>生成モデルがわかったので，観測データからパラメータを推定する方法を考えます．</p>

<p>まず，観測単語とトピック，トピック分布の同時確率関数は以下のようになります．</p>

<div style="overflow-x: auto;">
$$p(w, z, \theta) = p(w|z) p(z| \theta) p( \theta| \alpha)$$
</div>

<p>よって，文書$\mathbf{w} = w_1, w_2, &hellip; , w_N$について</p>

<div style="overflow-x: auto;">

$$p(\mathbf{w}, z, \theta) = p( \theta| \alpha) \prod_n p(w_n |z_n) p(z_n| \theta)$$

\begin{aligned}
p(\mathbf{w}) &= \int \sum_z p(\mathbf{w}, z, \theta) d \theta \\
&= \frac{\Gamma{(\sum_k \alpha_k})}{\prod_k \Gamma{(\alpha_k)}} \int (\prod_k \theta_k^{\alpha_k - 1}) \prod_n \sum_k p(w_n | k) \theta_k d \theta
\end{aligned}

</div>

<p>となり，これが尤度関数となります．この尤度を最大にするときのパラメータを推定したいです．</p>

<p>推定方法はいくつかありますが，今回はGibbsサンプラーを用いて<sup class="footnote-ref" id="fnref:1"><a href="#fn:1">1</a></sup>推定しました．</p>

<h2 id="ldaのgibbs-sampler">LDAのGibbs Sampler</h2>

<p>Gibbsサンプラーとは，マルコフ連鎖モンテカルロ法(MCMC)の最も簡単な場合で，潜在変数を分布ではなく，条件付き分布から実際にサンプリングしていくことでパラメータの推定を行う手法です．</p>

<p>潜在変数$z_1, z_2, &hellip;, z_N$を持つ確率モデル</p>

<p>$$p(X, z_1, z_2, &hellip;, z_N)$$</p>

<p>がある時，各$z_i$を考え直す，つまり条件付き分布</p>

<div style="overflow-x: auto;">
z_i \sim p(z_i | X, z_1, z_2, ..., z_{i-1}, z_{i+1}, ..., z_N)
</div>

<p>からランダムにサンプリングすることを繰り返します．</p>

<p>また，EMアルゴリズムとは違い，原理的に無限回繰り返せば真の分布からのサンプルになります．つまり，EMアルゴリズムでは，局所最適に陥る可能性が考えられますが，Gibbsサンプリングでは，大域的局所解を得やすいということになります．</p>

<p>LDAの潜在変数は，文書のトピック分布$\theta$と各単語のトピック$z$，トピック毎の単語の生起確率分布$\beta$ですが，ここでは$\theta$と$\beta$を積分消去して，尤もらしい$z$得ることを考えます．つまり</p>

<div style="overflow-x: auto;">
$$z_i \sim p(z_i | \mathbf{w}, z_{-i}, \alpha, \eta)$$
</div>

<p>から$z_i$をサンプリングして更新していくことで，尤もらしい$z_i$を得ます．</p>

<p>ここで，$p(z<em>i | \mathbf{w}, z</em>{-i}, \alpha, \eta)$は，</p>

<div style="overflow-x: auto;">

\begin{aligned}

\end{aligned}

</div>

<h2 id="参照">参照</h2>

<ul>
<li><a href="http://chasen.org/~daiti-m/paper/daichi15topicmodel-for-ecology.pdf">統計数理研究所 離散データの確率的トピックモデル</a>, 持橋大地, 生物に見られる時空間パターンと統計数理:同調・認知・行動にて (2015)</li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">Gibbsサンプラーは変分ベイズと比べて計算量が多いらしいですが，今回用いたデータは自分のブログの文書のみであり，リソースには余裕があったため，局所解に陥りにくいGibbsサンプラーを採用しました．
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>


        <h2 id="related">See Also</h2>

		
	</div>
     
	<div class="pagination">
		<a href="https://seiichiinoue.github.io/post/boostenv/" class="left arrow">&#8592;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			<span>
			&copy; <time datetime="2020-02-02 21:57:29.216607 &#43;0900 JST m=&#43;0.181804233">2020</time> Seiichi Inoue. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
            <script type="text/javascript" src="https://seiichiinoue.github.io/js/related.js"></script>
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		</footer>

    </body>
</html>
