<!DOCTYPE html>
<html lang="ja">
    <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>
				ガウス過程に基づく連続空間トピックモデルの実装 &middot; Seiichi Inoue
		</title>

		
  		<link rel="stylesheet" href="https://seiichiinoue.github.io/css/style.css">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
		
        
        
        
        
		
		<link href="" rel="alternate" type="application/rss+xml" title="Seiichi Inoue" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="https://seiichiinoue.github.io/">
					<h2 class="nav-title">Seiichi Inoue</h2>
				</a>
				<ul>
    <li><a href="https://seiichiinoue.github.io/about">About</a></li>
    <li><a href="https://seiichiinoue.github.io/">Posts</a></li>
</ul>

			</div>
		</nav>

        

<main>
	<div class="post">
		<div class="post-info">
        <br>
        <time datetime="2019-10-23 01:32:07 &#43;0900 JST">October 23, 2019</time>
</div>

		<h1 class="post-title">ガウス過程に基づく連続空間トピックモデルの実装</h1>
<div class="post-line"></div>

		

		<p>ガウス過程に基づく連続空間トピックモデル(CSTM)は，単語に潜在空間における座標を明示的に与え，その上にガウス過程を考えることによって，通常の混合モデルに基づくトピックモデルに比べて高精度な言語モデルを得ることのできる手法です．実装は<a href="https://github.com/seiichiinoue/cstm">seiichiinoue/cstm</a>にあります．</p>
<p>論文は<a href="http://chasen.org/~daiti-m/paper/nl213cstm.pdf">こちら</a>．</p>
<h2 id="トピックモデル">トピックモデル</h2>
<p>今回実装したCSTMは，トピックモデルの一種です．まずは，トピックモデルとはどのようなものなのか，簡単に説明をしてから本題に入ろうと思います．</p>
<p>トピックモデルは，様々な離散データに隠れた潜在的なトピックを推定するモデルです．ここでいうトピックとは，文書や会話における話題，分野などの大雑把な&quot;意味&quot;のようなものを表しています．</p>
<p>トピックモデルの代表であるLDA(Latent Dirichlet Allocation)において文書は，複数の潜在的なトピックから確率的に生成されていると仮定されており，また，単語はトピックが持つ確率分布に従って出現していると仮定しています．</p>
<p>つまり，以下のような過程で単語の集合である文書$w = w_1 w_2 \cdots w_N$が生成されたと仮定します．</p>
<ol>
<li>トピック(話題)分布 $\theta \sim Dir(\alpha)$ を生成</li>
<li>For n = 1 &hellip; N\
(a) トピック $z \sim Mult(\theta)$ を選択\
(b) 単語 $w_n \sim p(w | z)$ を生成</li>
</ol>
<p>ここで，$Mult(\theta)$は多項分布であり，$Dir(\alpha)$は多項分布の共役事前分布のディリクレ分布，$\theta$および，$\beta = { p(w | z) }$はLDAの学習パラメータです．</p>
<p>このような手法を用いて，文書の学習を行うと，文書集合のデータ(離散データ)から，文書に隠れた潜在的なトピックを機械的に推定することができ，文書ごとに選択されるトピックを見ることで，その文書を分類することに役立ちます．</p>
<p>ちなみに，LDAではトピックという潜在表現を用いて文書を表現していますが、CSTMでは直接単語の確率を操作しているので様々な拡張が可能になっています．</p>
<h2 id="cstmの考え方">CSTMの考え方</h2>
<p>CSTMでは，単語に潜在空間における座標を明示的に与えており，各単語$w$が$d$次元の潜在座標$\phi(w) \sim N(0, I_d)$を持っていると仮定します．つまり各単語は$d$次元のベクトルで表現され，そのベクトルのそれぞれの要素は，平均0，分散1の正規分布に従います．</p>
<!-- raw HTML omitted -->
<p>次に，意味的に関連のある単語の確率を同時に大きくするために，上図のようなカーネル行列$K$，平均が$0$のガウス過程</p>
<!-- raw HTML omitted -->
<p>を文書ごとに考え，文書$d$における単語の確率を以下のようにモデル化します．</p>
<!-- raw HTML omitted -->
<p>ここで，$f_d$は，実際には次元数が語彙数と同じガウス分布で，単語$w_k$に対応する$f_d(w_k)$は異なる値になりますが，平均は$0$になります．また$G_0(w_k)$は単語$w_k$のデフォルト確率を表しており，最尤推定値</p>
<!-- raw HTML omitted -->
<p>と考えます．また，CSTMでは，カーネル行列の要素として線形カーネル（実際には単語を単語座標へと射影する関数？）を用います．</p>
<!-- raw HTML omitted -->
<p>単語の確率となる$p_d (w_k)$は，文書ごとの倍率である$e^{f_d (w_k)}$とデフォルト確率$G_0(w_k)$を掛け合わせたものと解釈することができ，論文によると$f_d (w_k)$はおおよそ$-9 \leq f_d (w_k) \leq 9$をとるため，$e^{f_d (w_k)}$は$e^{-9} \leq e^{f_d (w_k)} \leq e^9$の範囲の値になるらしいです．</p>
<p>このようにモデル化することで，文書全体ではほとんどは出現しないが，特定の文書にだけ高頻度で出現するような単語であっても文書ごとに確率を変動させることで適切な確率を与えることができます．</p>
<p>しかし，実際のところ，言語には単語が一度出現すると，その後現れやすくなるというバースト性があり，この影響をモデル化するために，単語の確率に，$p_d (w_k)$ではなく，次のようなDirichlet Compound Multinomial(DCM)を用います．</p>
<!-- raw HTML omitted -->
<p>ここで，語彙数を$V$とすると，$n_d = (n_{d, 1}, n_{d, 2}, &hellip; , n_{d, V})$は文書$d$での各単語の出現頻度です．また，$w = (w_1, w_2, &hellip;, w_V)$は全ての単語です．</p>
<p>また，上式は語彙全体の文書$d$における同時確率を表しているため，その文書に含まれていない単語の確率も考えていることに注意です．</p>
<h2 id="学習">学習</h2>
<p>上述の$f$を直接求めるのは難しいため，補助変数を導入した手法を用います．</p>
<p>まず，文書$d$の潜在座標を$u_d \sim N(0, I_d)$とし，全ての単語の$\phi (w)$をまとめて以下のようにおきます．</p>
<!-- raw HTML omitted -->
<p>次に，$f_d = \Phi u_d$として，$u$を積分消去すると，（$f_d$の分散は，$u$の定義により，$E(\Phi u u^T \Phi^T) = \Phi^T E(u u^T) \Phi = \Phi I \Phi^T = K$となり）</p>
<!-- raw HTML omitted -->
<p>となり，この$f$は最初に定義したガウス過程と同じガウス過程に従います．</p>
<p>また，上述のDCMのパラメータである$\alpha_d$は，$f_d (w_k)$を用いて，以下のように表すことができます．</p>
<!-- raw HTML omitted -->
<p>このように，CSTMでは，語彙全体の同時確率をモデル化し，その確率を$\alpha$を通じて文書ごとに異なる値に変えるようなプロセスになっています．</p>
<p>CSTMにおける学習は，単語の確率$p_d$を最大化する文書ベクトル$u_d$と単語ベクトルの集合$\Phi$を更新していくことです．</p>
<p>学習方法として，単語の確率を微分して更新量を計算できそうですが，論文によると，$\phi (w_k)$の間には非常に高い相関があることから，局所解の問題のないランダムウォークによるメトロポリス・ヘイスティング法の使用が推奨されています．</p>
<p>MH法は，更新したい変数について，提案分布から候補となる値を生成し，採択確率に従ってその値で更新するか否かを決定し，更新していくアルゴリズムです．</p>
<p>MH法での更新において，文書ベクトル$u_d$の提案分布は，$N(u_d, \sigma^2_{(u)})$，単語ベクトル$\phi (w_k)$の提案分布は$N(\phi(w_k), \sigma^2_{(\phi)})$，$\alpha_0$の提案分布は$N(0, \sigma^2_{(\alpha)})$を使います．</p>
<p>これは書き直すと，$u^{\prime}_d \leftarrow u_d + N(0, \sigma^2_{(u)})$となり，現在のベクトルの各要素に正規分布か発生させたノイズを載せたものを新しい値とすることになります．</p>
<p>論文によると，$\sigma^2_{(u)} = 0.01$，$\sigma^2_{(\phi)} = 0.02$，$\sigma^2_{(\alpha)} = 0.2$です．</p>
<p>採択確率は，「パラメータの事前分布及び尤度を用いる」と論文には記載されていたのですが，詳細は載っていなかったので，<a href="http://musyoku.github.io/">ご注文は機械学習ですか</a>さんを参考にさせていただくと，以下のような形になると考えられます．</p>
<!-- raw HTML omitted -->
<p>単語ベクトルの採択確率は，以下のようになります．</p>
<!-- raw HTML omitted -->
<p>文書ベクトルの時とは違い，$\phi(w_k)$は値を変更すると全ての文書の確率に影響を与えるため，総乗が入ります．</p>
<p>$\alpha_0$の採択確率も，全文書での確率を用います．</p>
<!-- raw HTML omitted -->
<p>これらの採択確率を用いて，提案分布から生成した新しい値で確率的に更新していくことで，モデルの学習を行うことができます．</p>
<h2 id="実験">実験</h2>
<p>実験にはNIPS, CSJ, 毎日新聞の3つを用いました．以下にパープレキシティの推移を掲載します．</p>
<!-- raw HTML omitted -->
<p>いずれのコーパスに対しても推論ができていて，パープレキシティも収束していることがわかります．</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>まだ，とりあえず学習させてみただけなのですが，学習によって得られる潜在座標は，文書座標と同じ空間にあるので様々な可視化に役立ちそうと思いました．またこのモデルをある種の文書表現獲得器と捉えると，文書分類等の後続のタスクへの応用もできそうだなと思いました．</p>


        <h2 id="related">See Also</h2>

		
	</div>
     
	<div class="pagination">
		<a href="https://seiichiinoue.github.io/post/boost/" class="left arrow">&#8592;</a>
		<a href="https://seiichiinoue.github.io/post/vpylm/" class="right arrow">&#8594;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			<span>
			&copy; <time datetime="2023-11-16 23:59:58.813038 &#43;0900 JST m=&#43;0.258435909">2023</time> Seiichi Inoue. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
            <script type="text/javascript" src="https://seiichiinoue.github.io/js/related.js"></script>
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		</footer>

    </body>
</html>
