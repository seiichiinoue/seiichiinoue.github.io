<!DOCTYPE html>
<html lang="ja">
    <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>
				Hidden Markov Modelの実装 &middot; いのうの作業ログ
		</title>

		
  		<link rel="stylesheet" href="https://seiichiinoue.github.io/css/style.css">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
		
		<link rel="icon" type="image/png" sizes="32x32" href="https://seiichiinoue.github.io/img/favicon.png">
        
		<link rel="apple-touch-icon" sizes="180x180" href="https://seiichiinoue.github.io/img/favicon.png">
        
		
		<link href="" rel="alternate" type="application/rss+xml" title="いのうの作業ログ" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="https://seiichiinoue.github.io/">
					<h2 class="nav-title">いのうの作業ログ</h2>
				</a>
				<ul>
    <li><a href="https://seiichiinoue.github.io/about">About</a></li>
    <li><a href="https://seiichiinoue.github.io/">Posts</a></li>
</ul>

			</div>
		</nav>

        

<main>
	<div class="post">
		<div class="post-info">
        <br>
        <time datetime="2019-09-24 01:11:56 &#43;0900 JST">September 24, 2019</time>
</div>

		<h1 class="post-title">Hidden Markov Modelの実装</h1>
<div class="post-line"></div>

		

		

<p>隠れマルコフモデルとviterbiアルゴリズムを使用した教師なし品詞推定の実装を行いました．実装は<a href="https://github.com/seiichiinoue/hmm">seiichiinoue/hmm</a>に公開しています．</p>

<h2 id="品詞推定">品詞推定</h2>

<p>品詞の推定とは文$Y$が与えられた時の品詞列$X$を予測するものです．</p>

<p>実際には，解決策は多くあり，各単語を個別に予測する点予測では，パーセプトロンを用いた<a href="http://www.phontron.com/kytea/">KyTea</a>などがあります．また，系列に対する生成モデルには，隠れマルコフモデルを用いた，<a href="https://chasen-legacy.osdn.jp/">ChaSen</a>などがあります．今回は後者の隠れマルコフモデルを用いた品詞推定の実装を行いました．</p>

<p>モデルでは，文が与えられた場合の最も確率の高いタグ列を計算します．</p>

<div style="overflow-x: auto;">
$$argmax_{X} P(X | Y)$$
</div>

<p>これをベイズ則で分解します．</p>

<div style="overflow-x: auto;">
\begin{aligned}
argmax_{X} P(X | Y) &= argmax_{X} \frac{P(Y|X)P(X)}{P(Y)} \\
&= argmax_{X} P(Y|X)P(X)
\end{aligned}
</div>

<p>すると，第2式の$P(Y|X)$は，単語と品詞の関係を考慮しており，$P(X)$は前の品詞と次の品詞の関係を考慮しているものと解釈することができます．</p>

<p>これが系列に対する生成モデルになります．</p>

<h2 id="隠れマルコフモデル-hmm">隠れマルコフモデル(HMM)</h2>

<p>マルコフモデルは複数の状態を持ち，ある状態から別の状態へ一定の確率で遷移します(天気の例とかでみたことがあると思います)．この確率を遷移確率と呼び，状態の遷移後，その状態に依存した一定の確率で出力記号を出力します．この確率を出力確率と呼びます．</p>

<p>図で表すと以下のような動作になります．</p>

<p><img src="https://seiichiinoue.github.io/img/markov_model.png"></p>

<p>まず，初期状態を表す特殊な状態から$s_{t-1}$に遷移します．遷移したら$p(y|s_{t-1})$から出力記号を生成し，出力します．</p>

<p>次に，$p(s_t|s_{t-1})$に従って遷移先を決定します．遷移したら$p(y|s_t)$から出力記号を生成し，出力します．</p>

<p>今回は品詞推定を扱うので，状態$s$が品詞に対応し，出力記号$y$が単語に対応します．</p>

<p>品詞から品詞への遷移確率は，</p>

<div style="overflow-x: auto;">
$$ P(S) \approx \prod_{i=1}^{l+1} P_T (s_i | s_{i-1}) $$
</div>

<p>となり，品詞から単語の出力確率は，</p>

<div style="overflow-x: auto;">
$$ P(Y|S) \approx \prod_{i=1}^{l} P_E (y_i | s_i) $$
</div>

<p>となります．</p>

<p>またHMMでは，遷移確率$A$と出力確率$B$を次のように表します．</p>

<div style="overflow-x: auto;">
\begin{align}
    A = \begin{pmatrix}
            0.1 &0.7 &0.2 \\
            0.2 &0.1 &0.7 \\
            0.7 &0.2 &0.1 \\
        \end{pmatrix}\
    B = \begin{pmatrix}
            0.9 &0.1 \\
            0.6 &0.4 \\
            0.1 &0.9 \\
        \end{pmatrix}\
\end{align}
</div>

<p>ここでは状態数を3，出力記号数を2としています．実際には上述の通り，状態数は品詞の数になり，出力記号数は，単語数になります．</p>

<p>（遷移確率があらかじめ与えられているのはHMMモデルではなく，ただのマルコフモデルなので，ここでは，上の行列は推定した遷移確率行列と考えてください．）</p>

<p>この行列は行が品詞を表しており，列が遷移先の品詞と出力記号に対応しています．</p>

<p><img src="https://seiichiinoue.github.io/img/markov_transition.png"></p>

<p>簡単のため，図中では時刻$t$を表す添字をつけてありますが，実際は時刻に関係なく同じ行列を使用します．</p>

<h2 id="hmmの学習">HMMの学習</h2>

<p>以下，HMMのパラメータを推定するにあたって，モデルを表現するための記号を整理しておきます．</p>

<ul>
<li>$T$ : 観測回数</li>
<li>$c$ : 状態数</li>
<li>$m$ : 出力記号の数</li>
<li>$\omega_i$ : $i$番目の状態</li>
<li>$\upsilon_k$ : $k$番目の出力記号</li>
<li>$x_t \in (\omega_0, \omega_1, \cdots, \omega_{c-1} )$ : 時点$t$での状態</li>
<li>$Y_t \in (\upsilon_0, \upsilon_1, \cdots, \upsilon_{m- 1} )$ : 時点$t$での観測結果</li>
<li>$X = x_0 x_1 \cdots x_t \cdots x_{T-1}$ : 状態系列</li>
<li>$Y = Y_0 Y_1 \cdots Y_t \cdots Y_{T-1}$ : 出力記号系列</li>
<li>$a_{\omega_i \omega_j}$ : 状態$\omega_i$から状態$\omega_j$への遷移確率</li>
<li>$b_{\omega_j}(\upsilon_k)$ : 状態$\omega_j$で$\upsilon_k$を出力する確率</li>
<li>$\pi_i$ : 初期状態が$\omega_i$である確率</li>
<li>$A$ : $a_{\omega_i \omega_j}$を($i, j$)成分としてもつ$c \times c$の行列</li>
<li>$B$ : $b_{\omega_j}(\upsilon_k)$を($j, k$)成分としてもつ$c \times m$の行列</li>
<li>$\pi$ : $\pi_i$を成分としてもつ行列</li>
</ul>

<p>HMMの学習は，大きく分けて3つの問題を解くことによって実現します．</p>

<p>一つ目は，パラメータを既知としたとき，出力記号列としての観測結果$Y$が得られる確率を求める評価の問題です．二つ目は，パラメータを既知としたとき，観測結果$Y$を導く可能性の最も高い状態系列$X$を求める復号の問題です．そして三つ目が，パラメータを未知としたとき，観測結果から未知パラメータを求める推定の問題です．</p>

<p>これらを順番に説明していきます．</p>

<h3 id="出力確率の計算">出力確率の計算</h3>

<p>$Y = (Y_0 Y_1 ,&hellip;, Y_{T-1})$は出力記号列であり，$\lambda = (A, B, \pi)$ ($\pi$は事前分布)を与えられたモデルとすると，出力確率が知りたいので，求めたい値は$P(Y | \lambda)$になります．</p>

<p>$X = (x_0 x_1 ,&hellip;, x_{T-1})$を状態列とすると．出力確率$B$の定義より，</p>

<div style="overflow-x: auto;">
$$P(Y|X, \lambda) = b_{x_0}(Y_0) b_{x_1}(Y_1) \cdots b_{x_{T-1}}(Y_{T-1})$$
</div>

<p>となります．また，$\pi$と$A$の定義により，</p>

<div style="overflow-x: auto;">
$$P(X| \lambda) = \pi_{x_0} a_{x_0, x_1} a_{x_1, x_2} \cdots a_{x_{T-2}, x_{T-1}}$$
</div>

<p>であるので，</p>

<div style="overflow-x: auto;">
$$\begin{eqnarray}
P(Y, X | \lambda) &=& \frac{P(Y \cap X \cap \lambda)}{P(\lambda)} \\
P(Y|X, \lambda) P(X| \lambda) &=& \frac{P(Y \cap X \cap \lambda)}{P(X \cap \lambda)} \cdot \frac{P(X \cap \lambda)}{P(\lambda)} = \frac{P(Y \cap X \cap \lambda)}{P(\lambda)}
\end{eqnarray}$$
</div>

<p>となり，</p>

<div style="overflow-x: auto;">
$$P(Y, X | \lambda) = P(Y| X, \lambda) P(X| \lambda)$$
</div>

<p>を得ます．そして，これを$X$について周辺化することで，以下のように$P(Y| \lambda)$を得ることができます．</p>

<div style="overflow-x: auto;">
$$\begin{eqnarray}
P(Y|\lambda) &=& \Sigma_X P(Y, X | \lambda) \\
&=& \Sigma_X P(Y | X, \lambda) P(X | \lambda) \\
&=& \Sigma_X \pi_{x_0} b_{x_0}(Y_0) a_{x_0, x_1} b_{x_1} (Y_1) \cdots a_{x_{T-2}, x_{T-1}} b_{x_{T-1}} (Y_{T - 1})
\end{eqnarray}$$
</div>

<p>しかし，これを計算するには，膨大な時間がかかるため($O(TN^T)$)，HMMでは，forwardアルゴリズムを使用して計算します．</p>

<p>forwardアルゴリズムでは，$t = 0, 1 ,&hellip;, T-1$と，$i=0, 1 ,&hellip;, N-1$に対して，</p>

<div style="overflow-x: auto;">
$$\alpha_t (i) = P(Y_0, Y_1,...,Y_t, x_t = \omega_i | \lambda)$$
</div>

<p>を用います．$\alpha(i)$はマルコフ状態が$\omega_i$である$t$時点までに観測された文字列の確率です．</p>

<p>$\alpha(i)$は以下のように計算されます．</p>

<ol>
<li>$i = 0, 1, &hellip;, N-1$ に対して， $\alpha_0(i) = \pi_i b_i (Y_0)$ とする．</li>
<li>$t = 0, 1, &hellip;, T-1$ と $i = 0, 1, &hellip;, N-1$ に対して，以下を計算する．</li>
</ol>

<div style="overflow-x: auto;">

$$\alpha_t(i) = \biggl[ \sum_{j=0}^{N-1} \alpha_{t-1}(j) a_{ji} \biggr] b_i (Y_t) $$
</div>

<p>$T-1$までこの作業を行うと，以下のように，出力確率が算出されます．</p>

<div style="overflow-x: auto;">
$$ P(Y| \lambda) = \sum_{i=0}^{N-1} \alpha_{T-1}(i)$$
</div>

<p>以上のforwardアルゴリズムを使用することで，$O(TN^T)$だった計算量が，$O(N^2T)$になります．</p>

<h3 id="隠れ状態の推定">隠れ状態の推定</h3>

<p>モデルと文字列が与えられた時，最も尤もらしい隠れ状態の列を求めるために，HMMではbackwardアルゴリズムを使用して計算します．</p>

<p>backwardアルゴリズムでは，出力確率の計算の時と同様に$t = 0, 1, &hellip;, T-1$と，$i=0, 1, &hellip;, N-1$に対して，</p>

<div style="overflow-x: auto;">
$$\beta_t(i) = P(Y_{t+1}, Y_{t+2}, Y_{T-1}, x_t = \omega_i | \lambda)$$
</div>

<p>を定義します．$\beta(i)$は以下のように計算されます．</p>

<ol>
<li>$i = 0, 1, &hellip;, N-1$に対して，$\beta_{T-1}(i) = 1$とする．</li>
<li>$t = t-2, t-3, &hellip;, 0$ と $i=0, 1, &hellip;, N-1$に対して，以下を計算する．</li>
</ol>

<div style="overflow-x: auto;">
$$\beta_t(i) = \sum_{j=0}^{N-1} a_{ij} b_j (Y_{t+1}) \beta_{t+1}(j)$$
</div>

<p>ここで，$t = 0, 1, &hellip;, T-1$ と $i=0, 1, &hellip;, N-1$に対して，</p>

<div style="overflow-x: auto;">
$$\gamma_t(i) = P(x_t = \omega_i | Y, \lambda)$$
</div>

<p>を定義します．また，$\alpha_t(i)$は$t$時点までの確率，$\beta_t(i)$は$t$時点以降の確率になっていることから，</p>

<div style="overflow-x: auto;">
$$\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{P(Y | \lambda)}$$
</div>

<p>であり（分母の$P(Y | \lambda)$は$\alpha_{T-1}(i)$を$i$で周辺化すれば得られます），この$\gamma$を最大化するように隠れ状態を決定すれば良いことになります．</p>

<p>しかし，この方法ではマルコフ性を考慮した系列とならない場合があるので，後に説明するviterbiアルゴリズムを用いて計算します．</p>

<h3 id="モデルの推定">モデルの推定</h3>

<p>出力確率と隠れ状態を算出できたので，それらを用いて，観測データに最もフィットするモデルのパラメータの調整を行います．</p>

<p>まず，$t = 0, 1, &hellip;, T-2$と，$i, j \in (0, 1, &hellip;, N-1)$に対して，以下の&rdquo;di-gamma&rdquo;を定義します．</p>

<div style="overflow-x: auto;">
$$\gamma_t(i, j) = P(x_t = \omega_i, x_{t+1} = \omega_j | Y, \lambda)$$
</div>

<p>よって$\gamma(i, j)$は，時刻$t$に$\omega_i$となり，時刻$t+1$に$\omega_j$に遷移する確率です．そしてdi-gammaは$\alpha, \, \beta, \, A, \, B$を用いて，以下のように表すことができます．</p>

<div style="overflow-x: auto;">
$$\gamma_t(i, j) = \frac{\alpha_t(i) a_{ij} b_j (Y_{t+1}) \beta_{t+1} (j)}{P(Y | \lambda)}$$
</div>

<p>また，$t = 0, 1, &hellip;, T-2$において$\gamma_t(i)$と$\gamma_t(i, j)$の間には以下の関係があります．</p>

<div style="overflow-x: auto;">
$$\gamma_t(i) = \sum_{j=0}^{N-1} \gamma_t(i, j)$$
</div>

<p>$\gamma$とdi-gammaが与えられたので，$\lambda = (A, B, \pi)$は以下のように推定することができます．</p>

<p>$i = 0, 1, &hellip;, N-1$に対して，</p>

<div style="overflow-x: auto;">
$$\pi_i = \gamma_0(i)$$
</div>

<p>$i = 0, 1, &hellip;, N-1$と，$j = 0, 1, &hellip;, N-1$に対して以下のように$a_{ij}$を計算する</p>

<div style="overflow-x: auto;">
$$a_{ij} = \sum_{t=0}^{T-2} \gamma_t(i, j) / \sum_{t=0}^{T-2} \gamma_t(i)$$
</div>

<p>$j = 0, 1, &hellip;, N-1$と$k = 0, 1, &hellip;, M-1$に対して以下のように$b_j(k)$を計算する</p>

<div style="overflow-x: auto;">
$$b_j(k) = \sum_{t \in (0, 1, ..., T-1), \, Y = k} \gamma_t(j) / \sum_{t=0}^{T-1} \gamma_t(j)$$
</div>

<p>$a_{ij}$の推定式の分子は，状態$\omega_i$から$\omega_j$への遷移の期待値を表しており，分母は，状態$\omega_i$からあらゆる状態への遷移の期待値を表しています．</p>

<p>また，$b_{jk}$の推定式の分子は，$Y=k$の時にモデルが隠れ状態$\omega_j$だった回数の期待値，分母は，モデルが隠れ状態$\omega_j$だった回数の期待値を表しています．</p>

<p>これまでの過程をまとめると，モデルの推定は以下のように行うことができます．</p>

<ol>
<li><p>$\lambda = (A, B, \pi)$を初期化</p></li>

<li><p>$\alpha_t(i), \, \beta_t(i), \, \gamma_t(i, j), \, \gamma_t(i)$を計算する</p></li>

<li><p>$\lambda = (A, B, \pi)$を再推定する</p></li>

<li><p>$P(Y | \lambda)$が上昇すれば，2に戻る．</p></li>
</ol>

<h2 id="viterbiアルゴリズム">Viterbiアルゴリズム</h2>

<p>viterbiアルゴリズムは，モデル$\lambda$において最適な状態系列（最適経路）と，その経路上での確率を求める動的計画法を用いたアルゴリズムです．</p>

<p>まずモデル$\lambda$において，連続した観測系列$Y = y_1 y_2 &hellip; y_T$に対する最適な系列$s = s_1 s_2 &hellip; s_T$を求めるために，時刻$t$で状態$i$に至るまでの最適状態確率$\delta_t(i)$を定義します．</p>

<div style="overflow-x: auto;">
$$\delta_t(i) = max_{s_1, s_2, ..., s_{t-1}} \, p(s_1, s_2, ..., s_t = i, \, y_1, y_2, ..., y_t| \lambda)$$
</div>

<p>また，$t$時点における最適状態確率は，次のように導出できます．</p>

<div style="overflow-x: auto;">
$$\delta_{t}(j) = \bigl[ max_{i} \delta_{t-1}(i) a_{ij} \bigr] \cdot b_{j}(y_{t})$$
</div>

<p>ちなみに，これはforwardアルゴリズムで導出した式とほとんど一緒で，異なるのは，forwardアルゴリズムでは，合計をとっていたところを，viterbiアルゴリズムでは，最大値をとっているところです．</p>

<p>$t$時点，状態$i$において，出力確率を最大にする経路（状態遷移）を$\phi_t(j)$，最適経路の出力確率を$P^*$，最適経路上の最終状態を$s_T^*$とすると，最適経路および出力確率は以下のようにして求めることができます．</p>

<p>$\delta$と$\phi$を初期化する</p>

<div style="overflow-x: auto;">
$$\delta_0(i) = \pi_i$$
$$\phi_0(i) = 0$$
</div>

<p>それぞれを計算する</p>

<div style="overflow-x: auto;">
$$\delta_t(j) = max_{1 \leq i \leq N-1} \bigl[ \delta_{t-1}(i) a_{ij} b_{j} (y_t) \bigr]$$
$$\phi_t(j) = argmax_{1 \leq i \leq N-1} \bigl[ \delta_{t-1}(i) a_{ij} b_{j} (y_t) \bigr]$$
</div>

<p>結果を格納する</p>

<div style="overflow-x: auto;">
$$P^* = max_{1 \leq i \leq N-1} \bigl[ \delta_T(i) \bigr]$$
$$s_T^* = argmax_{1 \leq i \leq N-1} \bigl[ \delta_T(i) \bigr]$$
</div>

<p>状態系列を復元する</p>

<div style="overflow-x: auto;">
$$s_t^* = \phi_{t+1} (s_{t+1}^*) \,\, (0 \leq t \leq T - 1)$$
</div>

<h2 id="実験">実験</h2>

<p>隠れ状態=30，イテレーション=200で学習を行いました．</p>

<p>以下はwikipedia日本語コーパスを使用して，実際に学習させた結果です．正解率は67.1%でした．</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">回転/16 数/25 の/5 制御/29 が/3 事実/16 上/25 無/11 段階/28 で/6 可能/23 で/6 あ/2 る/21 ため/3 、/11 加速/13 ・/5 減速/16 時/25 の/5 衝動/29 を/6 軽減/14 でき/2 る/21 。/12 ハートフル/0 コミュニケーション/9 。/12
俗称/15 と/6 し/10 て/19 、/11 セルビア/13 ・/11 モンテネグロ/28 連邦/15 と/6 呼/14 ば/18 れ/10 る/21 こと/3 も/6 あ/2 る/21 。/12
死者/0 の/5 日/8 　/20 キリスト/24 教/20 （/22 カトリック/1 ）/8 の/26 記念/1 日/8 。/12
水沢/0 江刺/9 ～/22 新/11 花巻/28 間/15 の/26 数/27 箇所/29 で/6 東北/11 新/28 幹線/15 の/26 橋梁/7 の/5 柱/29 の/26 外壁/23 が/6 剥げ/13 、/11 鉄骨/28 が/6 むき出し/23 に/6 な/18 っ/10 た/21 。/12
ゲーム/15 ボーイ/26 アドバンス/7 用/26 ソフト/17 。/12
また/0 これ/5 ら/29 地学/3 的/6 な/27 現象/29 のみ/6 な/18 ら/10 ず/19 、/11 陸上/13 古/11 生物/28 の/26 分布/27 状況/29 など/3 も/25 、/11 「/27 大陸/29 が/6 動/14 い/2 て/19 離合/23 集散/6 し/10 た/21 」/3 状況/5 証拠/29 と/6 さ/18 れ/10 て/19 い/2 る/21 。/12
死者/0 の/5 霊魂/29 の/26 ため/7 に/6 祈り/23 を/6 捧げ/2 る/21 日/3 。/12
第/0 １１/1 条/8 ：/22 俘虜/16 は/25 宣誓/11 解放/28 の/26 受諾/23 を/6 強制/14 さ/18 れ/10 る/21 こと/3 な/5 く/13 、/11 また/16 敵/25 の/5 政府/16 は/25 宣誓/11 解放/28 を/6 求め/2 る/21 俘虜/3 の/6 請願/23 に/6 応/14 ずる/17 義務/12 は/14 な/2 い/21 。/12</code></pre></div>
<p>以下は我輩は猫であるの全文の学習結果です．正解率は78.8%でした．</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">吾輩/12 は/20 猫/3 で/5 ある/13 。/11
名前/12 は/20 まだ/3 無い/13 。/11
どこ/3 で/5 生れ/17 た/0 か/26 とんと/14 見当/9 が/23 つか/27 ぬ/0 。/11
何/12 でも/20 薄暗い/14 じめじめ/3 し/17 た/0 所/18 で/5 ニャーニャー/17 泣い/17 て/19 いた事/10 だけ/29 は/20 記憶/3 し/17 て/19 いる/10 。/11
吾輩/12 は/20 ここ/3 で/5 始め/17 て/19 人間/4 という/7 もの/18 を/23 見/27 た/0 。/11
しかも/28 あと/3 で/5 聞く/13 と/24 それ/12 は/20 書生/4 という/7 人間/8 中/18 で/5 一番/20 獰悪/4 な/7 種族/18 で/5 あっ/17 た/0 そう/29 だ/25 。/11
この/28 書生/4 という/7 の/29 は/20 時々/14 我々/9 を/23 捕え/27 て/19 煮/27 て/19 食う/4 という/7 話/18 で/5 ある/13 。/11
しかし/28 その/1 当時/12 は/20 何/4 という/7 考/18 も/23 なかっ/27 た/0 から/26 別段/14 恐し/6 いとも/24 思わ/22 なかっ/17 た/0 。/11
ただ/28 彼/4 の/7 掌/18 に/5 載せ/17 られ/17 て/19 スー/16 と/24 持ち上げ/22 られ/17 た/0 時/26 何だか/14 フワフワ/3 し/17 た/0 感じ/18 が/23 あっ/27 た/0 ばかり/2 で/5 ある/13 。/11</code></pre></div>
<p>以下はこころの全文の学習結果です．正解率は83.5%でした．</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">私/6 は/11 その/8 人/1 を/3 常に/24 先生/25 と/4 呼ん/9 で/0 い/26 た/15 。/19
だから/21 ここ/6 でも/11 ただ/27 先生/25 と/4 書く/10 だけ/1 で/0 本名/22 は/22 打ち明け/22 ない/20 。/19
これ/6 は/11 世間/16 を/3 憚/17 かる/10 遠慮/1 と/4 いう/10 より/1 も/0 、/24 その/8 方/1 が/0 私/6 にとって/11 自然/27 だ/2 から/4 で/0 ある/20 。/19
私/6 は/11 その/8 人/1 の/29 記憶/12 を/3 呼び/17 起す/10 ごと/1 に/0 、/24 すぐ/24 「/21 先生/27 」/2 と/4 いい/22 たく/23 なる/15 。/19
筆/16 を/3 執っ/18 て/13 も/5 心持/1 は/11 同じ/8 事/1 で/0 ある/20 。/19
よそよそしい/6 頭文字/11 など/6 は/11 とても/27 使う/10 気/9 に/7 なら/22 ない/20 。/19
私/6 が/11 先生/25 と/4 知り合い/9 に/7 なっ/18 た/15 の/14 は/11 鎌倉/9 で/0 ある/20 。/19
その/8 時/28 私/6 は/11 まだ/27 若々しい/8 書生/1 で/0 あっ/26 た/15 。/19</code></pre></div>
<p>初めての実装にしては，うまくいって満足でした．収束速度が遅かったり，未知語への対応としてのスムージングなどを行なっていなかったりとまだまだ改善点がありそうなので，少しずつやっていきたいと思います．</p>


        <h2 id="related">See Also</h2>

		
	</div>
     
	<div class="pagination">
		<a href="https://seiichiinoue.github.io/post/gcclink/" class="left arrow">&#8592;</a>
		<a href="https://seiichiinoue.github.io/post/scc/" class="right arrow">&#8594;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			<span>
			&copy; <time datetime="2020-02-04 12:12:37.54139 &#43;0900 JST m=&#43;0.202015813">2020</time> Seiichi Inoue. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
            <script type="text/javascript" src="https://seiichiinoue.github.io/js/related.js"></script>
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		</footer>

    </body>
</html>
