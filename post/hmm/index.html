<!DOCTYPE html>
<html lang="ja">
    <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>
				Hidden Markov Modelの実装 &middot; いのうの作業ログ
		</title>

		
  		<link rel="stylesheet" href="https://seiichiinoue.github.io/css/style.css">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
		
		<link rel="icon" type="image/png" sizes="32x32" href="https://seiichiinoue.github.io/images/favicon.png">
        
		<link rel="apple-touch-icon" sizes="180x180" href="https://seiichiinoue.github.io/images/favicon.png">
        
		
		<link href="" rel="alternate" type="application/rss+xml" title="いのうの作業ログ" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="https://seiichiinoue.github.io/">
					<h2 class="nav-title">いのうの作業ログ</h2>
				</a>
				<ul>
    <li><a href="https://seiichiinoue.github.io/about">About</a></li>
    <li><a href="https://seiichiinoue.github.io/">Posts</a></li>
</ul>
			</div>
		</nav>

        

<main>
	<div class="post">
		<div class="post-info">
        <br>
        <time datetime="2019-09-24 01:11:56 &#43;0900 JST">September 24, 2019</time>
</div>

		<h1 class="post-title">Hidden Markov Modelの実装</h1>
<div class="post-line"></div>

		

		

<p>隠れマルコフモデルとviterbiアルゴリズムを使用した教師なし品詞推定の実装を行いました．実装は<a href="https://github.com/seiichiinoue/hmm">レポジトリ</a>に公開しています．</p>

<h2 id="品詞推定">品詞推定</h2>

<p>品詞の推定とは文$X$が与えられた時の品詞列$Y$を予測するものです．</p>

<p>実際には，解決策は多くあり，各単語を個別に予測する点予測では，パーセプトロンを用いた<a href="http://www.phontron.com/kytea/">KyTea</a>などがあります．また，系列に対する生成モデルには，隠れマルコフモデルを用いた，<a href="https://chasen-legacy.osdn.jp/">ChaSen</a>などがあります．今回は後者の隠れマルコフモデルを用いた品詞推定の実装を行いました．</p>

<p>モデルでは，文が与えられた場合の最も確率の高いタグ列を計算します．</p>

<div style="overflow-x: auto;">
$$argmax_{Y} P(Y | X)$$
</div>

<p>これをベイズ則で分解します．</p>

<div style="overflow-x: auto;">
\begin{aligned}
argmax_{Y} P(Y | X) &= argmax_{Y} \frac{P(X|Y)P(Y)}{P(X)} \\
&= argmax_{Y} P(X|Y)P(Y)
\end{aligned}
</div>

<p>すると，第2式の$P(X|Y)$は，単語と品詞の関係を考慮しており，$P(Y)$は前の品詞と次の品詞の関係を考慮しているものと解釈することができます．</p>

<p>これが系列に対する生成モデルになります．</p>

<h2 id="隠れマルコフモデル-hmm">隠れマルコフモデル(HMM)</h2>

<p>マルコフモデルは複数の状態を持ち，ある状態から別の状態へ一定の確率で遷移します(天気の例とかでみたことがあると思います)．この確率を遷移確率と呼び，状態の遷移後，その状態に依存した一定の確率で出力記号を出力します．この確率を出力確率と呼びます．</p>

<p>図で表すと以下のような動作になります．</p>

<p><img src="http://musyoku.github.io/images/post/2017-02-27/markov_model.png"></p>

<p>まず，初期状態を表す特殊な状態から$s_{t-1}$に遷移します．遷移したら$p(y|s_{t-1})$から出力記号を生成し，出力します．</p>

<p>次に，$p(s_t|s_{t-1})$に従って遷移先を決定します．遷移したら$p(y|s_t)$から出力記号を生成し，出力します．</p>

<p>今回は品詞推定を扱うので，状態$s$が品詞に対応し，出力記号$y$が単語に対応します．</p>

<p>品詞から品詞への遷移確率は，</p>

<div style="overflow-x: auto;">
$$ P(Y) \approx \prod_{i=1}^{l+1} P_T (y_i | y_{i-1}) $$
</div>

<p>となり，品詞から単語の出力確率は，</p>

<div style="overflow-x: auto;">
$$ P(X|Y) \approx \prod_{1}^{l} P_E (x_i | y_i) $$
</div>

<p>となります．</p>

<p>またHMMでは，遷移確率$A$と出力確率$B$を次のように表します．</p>

<div style="overflow-x: auto;">
\begin{align}
    A = \begin{pmatrix}
            0.1 &0.7 &0.2 \\
            0.2 &0.1 &0.7 \\
            0.7 &0.2 &0.1 \\
        \end{pmatrix}\
    B = \begin{pmatrix}
            0.9 &0.1 \\
            0.6 &0.4 \\
            0.1 &0.9 \\
        \end{pmatrix}\
\end{align}
</div>

<p>ここでは状態数を3，出力記号数を2としています．実際には上述の通り，状態数は品詞の数になり，出力記号数は，単語数になります．</p>

<p>（遷移確率があらかじめ与えられているのはHMMモデルではなく，ただのマルコフモデルなので，ここでは，上の行列は推定した遷移確率行列と考えてください．）</p>

<p>この行列は行が品詞を表しており，列が遷移先の品詞と出力記号に対応しています．</p>

<p><img src="http://musyoku.github.io/images/post/2017-02-27/markov_transition.png"></p>

<p>簡単のため，図中では時刻$t$を表す添字をつけてありますが，実際は時刻に関係なく同じ行列を使用します．</p>

<h2 id="hmmの学習">HMMの学習</h2>

<h3 id="出力確率の計算">出力確率の計算</h3>

<p>$\lambda = (A, B, \pi)$ ($\pi$は事前分布)を与えられたモデルとし，$Y = (Y_0 Y_1 ,&hellip;, Y_{T-1})$を出力記号列とします．出力確率が知りたいので，求めたい値は$P(Y | \lambda)$になります．</p>

<p>$X = (x_0 x_1 ,&hellip;, x_{T-1})$を状態列とすると．出力確率$B$の定義より，</p>

<div style="overflow-x: auto;">
$$P(Y|X, \lambda) = b_{x_0}(Y_0) b_{x_1}(Y_1) \cdots b_{x_{T-1}}(Y_{T-1})$$
</div>

<p>となります．また，$\pi$と$A$の定義により，</p>

<div style="overflow-x: auto;">
$$P(X| \lambda) = \pi_{x_0} a_{x_0, x_1} a_{x_1, x_2} \cdots a_{x_{T-2}, x_{T-1}}$$
</div>

<p>であるので，</p>

<div style="overflow-x: auto;">
$$P(Y, X | \lambda) = \frac{P(Y \cap X \cap \lambda)}{P(\lambda)}$$

$$P(Y|X, \lambda) P(X| \lambda) = \frac{P(Y \cap X \cap \lambda)}{P(X \cap \lambda)} \cdot \frac{P(X \cap \lambda)}{P(\lambda)} = \frac{P(Y \cap X \cap \lambda)}{P(\lambda)}$$
</div>

<p>となり，</p>

<div style="overflow-x: auto;">
$$P(Y, X | \lambda) = P(Y| X, \lambda) P(X| \lambda)$$
</div>

<p>を得ます．これを$X$について周辺化することで，$P(Y| \lambda)$を得ることができます．</p>

<p>しかし，これを計算するには，膨大な時間がかかるため($O(TN^T)$)，HMMでは，forwardアルゴリズムを使用して計算します．</p>

<p>forwardアルゴリズムでは，$t = 0, 1 ,&hellip;, T-1$と，$i=0, 1 ,&hellip;, N-1$に対して，</p>

<div style="overflow-x: auto;">
$$\alpha_t (i) = P(Y_0, Y_1,...,Y_t, x_t = q_i | \lambda)$$
</div>

<p>を用います．$\alpha(i)$はマルコフ状態が$q_i$である$t$時点までに観測された文字列の確率です．</p>

<p>$\alpha(i)$は以下のように計算されます．</p>

<ol>
<li>$i = 0, 1, &hellip;, N-1$ に対して， $\alpha_0(i) = \pi_i b_i (Y_0)$ とする．</li>
<li>$t = 0, 1, &hellip;, T-1$ と $i = 0, 1, &hellip;, N-1$ に対して，以下を計算する．</li>
</ol>

<div style="overflow-x: auto;">

$$\alpha_t(i) = \biggl[ \sum_{j=0}^{N-1} \alpha_{t-1}(j) a_{ji} \biggr] b_i (Y_t) $$
</div>

<p>$T-1$までこの作業を行うと，以下のように，出力確率が算出されます．</p>

<div style="overflow-x: auto;">
$$ P(Y| \lambda) = \sum_{i=0}^{N-1} \alpha_{T-1}(i)$$
</div>

<p>以上のforwardアルゴリズムを使用することで，$O(TN^T)$だった計算量が，$O(N^2T)$になります．</p>

<h3 id="隠れ状態の推定">隠れ状態の推定</h3>

<p>モデルと文字列が与えられた時，最も尤もらしい隠れ状態の列を求めるために，HMMではbackwardアルゴリズムを使用して計算します．</p>

<p>backwardアルゴリズムでは，出力確率の計算の時と同様に$t = 0, 1, &hellip;, T-1$と，$i=0, 1, &hellip;, N-1$に対して，</p>

<div style="overflow-x: auto;">
$$\beta_t(i) = P(Y_{t+1}, Y_{t+2}, Y_{T-1}, x_t = q_i | \lambda)$$
</div>

<p>を定義します．$\beta(i)$は以下のように計算されます．</p>

<ol>
<li>$i = 0, 1, &hellip;, N-1$に対して，$\beta_{T-1}(i) = 1$とする．</li>
<li>$t = t-2, t-3, &hellip;, 0$ と $i=0, 1, &hellip;, N-1$に対して，以下を計算する．</li>
</ol>

<div style="overflow-x: auto;">
$$\beta_t(i) = \sum_{j=0}^{N-1} a_{ij} b_j (Y_{t+1}) \beta_{t+1}(j)$$
</div>

<p>$t = 0, 1, &hellip;, T-1$ と $i=0, 1, &hellip;, N-1$に対して，</p>

<div style="overflow-x: auto;">
$$\gamma_t(i) = P(x_t = q_i | Y, \lambda)$$
</div>

<p>を定義する．また，$\alpha_t(i)$は$t$時点までの確率，$\beta_t(i)$は$t$時点以降の確率になっていることから，</p>

<div style="overflow-x: auto;">
$$\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{P(Y | \lambda)}$$
</div>

<p>であり（分母の$P(Y | \lambda)$は$\alpha_{T-1}(i)$を$i$で周辺化すれば得られます），この$\gamma$を最大化するように隠れ状態を決定すれば良いことになります．</p>

<h3 id="モデルの推定">モデルの推定</h3>

<p>出力確率と隠れ状態を算出できたので，それらを用いて，観測データに最もフィットするモデルのパラメータの調整を行います．</p>

<p>まず，$t = 0, 1, &hellip;, T-2$と，$i, j \in (0, 1, &hellip;, N-1)$に対して，以下の&rdquo;di-gamma&rdquo;を定義します．</p>

<div style="overflow-x: auto;">
$$\gamma_t(i, j) = P(x_t = q_i, x_{t+1} = q_j | Y, \lambda)$$
</div>

<p>よって$\gamma(i, j)$は，時刻$t$に$q_i$となり，時刻$t+1$に$q_j$に遷移する確率です．そしてdi-gammaは$\alpha, \, \beta, \, A, \, B$を用いて，以下のように表すことができます．</p>

<div style="overflow-x: auto;">
$$\gamma_t(i, j) = \frac{\alpha_t(i) a_{ij} b_j (Y_{t+1}) \beta_{t+1} (j)}{P(Y | \lambda)}$$
</div>

<p>また，$t = 0, 1, &hellip;, T-2$において$\gamma_t(i)$と$\gamma_t(i, j)$の間には以下の関係があります．</p>

<div style="overflow-x: auto;">
$$\gamma_t(i) = \sum_{j=0}^{N-1} \gamma_t(i, j)$$
</div>


		
	</div>

	<div class="pagination">
		<a href="https://seiichiinoue.github.io/post/gcclink/" class="left arrow">&#8592;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			<span>
			&copy; <time datetime="2019-09-24 21:03:29.063398 &#43;0900 JST m=&#43;0.055593279">2019</time> Seiichi Inoue. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		</footer>

    </body>
</html>
