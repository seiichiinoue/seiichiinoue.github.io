<!DOCTYPE html>
<html lang="ja">
    <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>
				Pitman-Yor過程に基づく可変長n-gram言語モデルの実装 &middot; いのうの作業ログ
		</title>

		
  		<link rel="stylesheet" href="https://seiichiinoue.github.io/css/style.css">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
		
		<link rel="icon" type="image/png" sizes="32x32" href="https://seiichiinoue.github.io/img/favicon.png">
        
		<link rel="apple-touch-icon" sizes="180x180" href="https://seiichiinoue.github.io/img/favicon.png">
        
		
		<link href="" rel="alternate" type="application/rss+xml" title="いのうの作業ログ" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="https://seiichiinoue.github.io/">
					<h2 class="nav-title">いのうの作業ログ</h2>
				</a>
				<ul>
    <li><a href="https://seiichiinoue.github.io/about">About</a></li>
    <li><a href="https://seiichiinoue.github.io/">Posts</a></li>
</ul>
			</div>
		</nav>

        

<main>
	<div class="post">
		<div class="post-info">
        <br>
        <time datetime="2019-11-05 10:19:16 &#43;0900 JST">November 5, 2019</time>
</div>

		<h1 class="post-title">Pitman-Yor過程に基づく可変長n-gram言語モデルの実装</h1>
<div class="post-line"></div>

		

		

<p>前々回の品詞推定，前回のトピックモデルに続いて今回は言語モデルの実装を行いました（少し順序が前後してしまいましたが）．実装は<a href="https://github.com/seiichiinoue/vpylm">seiichiinoue/vpylm</a>です．</p>

<p>n-gram言語モデルとは，単語間のマルコフ過程によって文の確率を計算するもので，自然言語処理の様々な場面に適用されてきた，基礎的で重要なモデルです．</p>

<p>n-gram言語モデルは，直前の$n-1$個の単語列を状態とした$n-1$次のマルコフモデルによって次の単語の条件付き確率を計算していくものです．この時に，状態数は単語の総数を$V$とすると$V^{n-1}$のオーダーとなり，$n$を1増やすとと総パラメータ数は通常数万倍となり，指数的に爆発します．このため，せいぜい$n = 4, 5$程度が限界であり，それ以上の長い相関は計算問題上取り扱えないという問題がありました．</p>

<p>しかし，実際の言語データには，&rdquo;The United States of America&rdquo;のようなトライグラム（3-gram）を超える長いフレーズや固有名詞が頻出します．そういった問題を解決すべく，単語分割の粒度に依存しないモデルとして提案されたのが，今回のVPYLMです．</p>

<p>ちなみに，上の問題を理論的に解決できなかった理由は，nグラム分布を階層的に生成する確率モデルが存在しなかったためらしいです．しかし，VPYLMの元となっている，HPYLMによって提案された，階層Pitman-Yor過程とよばれるノンパラメトリックな確率過程によって，適切にスムージングされたnグラム分布を階層的に生成，推定できることが明らかになりました．</p>

<p>VPYLMは，Pitman-Yor過程に基づくn-gramモデル（HPYLM）を拡張して，データ中の各単語が生成されたnグラム長を隠れ変数とみなしてベイズ推定を行います．</p>

<h2 id="pitman-yor過程と階層化">Pitman-Yor過程と階層化</h2>

<p>Pitman-Yor過程は中華料理店過程の一般化です．まずは中華料理店過程とは何か，簡単に説明します．</p>

<p>中華料理店過程とは，ディリクレ過程の実現例の一つであり，クラスタリングの事前確率をモデルかする手法です．中華料理店に客がやってきたときに，どの円卓に座るかを考えることが名前の由来となっています．</p>

<p>店には無数の円卓があると仮定し，また円卓には無数の客が座れると仮定します．</p>

<p>まず，最初の客は空の円卓に座ります．2番目以降の客は座っている人の多い円卓を好んで着席するというルールに従います．</p>

<p>厳密に表現すると，以下のようになります．（下の図を参照しながら確認してください）</p>

<ul>
<li>すでに$n_i$人着席しているテーブル$i$に確率$n_i / (n - 1 + \theta)$で着席する</li>
<li>新しいテーブルに確率$\theta / (n - 1 + \theta)$で着席する</li>
</ul>

<p>これを自然言語で考えると，テーブルは単語，客はその単語の生成回数と考えることができ，ある単語を観測する以前の状態の時に，これから観測する単語が何なのかのモデリングに適用できます．以下は，7人目の客が座るテーブルを決定するそれぞれの確率を示しています．</p>

<p><img src="https://seiichiinoue.github.io/img/crp.png"></p>

<p>$n$人目までの客が観測された状態で，$n+1$人目の客が座るテーブルの予測分布を考えます．総テーブル数を$t$，総客数を$c$，$k$番目のテーブルの客数を$c_k$，$\theta$を集中度パラメータ，$G_0$を基底測度とすると，次の客$x_{n+1}$が座るテーブル$w$は，以下のようにモデル化することができます．</p>

<div style="overflow-x: auto;">
$$p(x_{n+1} | x_1 ... x_n) = \Sigma_{k=1}^{t} \frac{c_k}{c + \theta} + \frac{\theta}{c + \theta} G_0$$

$$p(x_{n+1} = w | \Theta) = \frac{c_w}{c + \theta} + \frac{\theta}{c + \theta}$$
</div>

<p>ここで，基底測度$G_0$は，未観測のテーブルを生成する親の連続分布であり，集中度パラメータ$\theta$について，$\theta$を大きくすると，$G_0$に近づくことに注意しましょう．（逆に$\theta$が小さいと経験分布$\frac{c_w}{c + \theta}$に近づきます）</p>

<p>このように，中華料理店過程は，集中度パラメータ$\theta$と基底測度$G_0$から新しい分布$G$を生成することができ，これを通常以下のように書きます．</p>

<div style="overflow-x: auto;">
$$G \sim DP(G_0, \theta)$$
</div>

<p>Pitman-Yor過程は，中華料理店過程にディスカウント係数$d$を足したものになっており，基本は中華料理店過程と変わりません．</p>

<p>着席の際，n番目以降の客は，</p>

<ul>
<li>すでに$n_i$人着席しているテーブル$i$に確率$(n_i - d) / (n - 1 + \theta)$で着席する．</li>
<li>新しいテーブルに確率$(\theta + d_t) / (n - 1 + \theta)$で着席する．</li>
</ul>

<p>のルールに基づいてテーブルに着席します．</p>

<p><img src="https://seiichiinoue.github.io/img/pitman-yor.png"></p>

<p>生成される分布は，</p>

<div style="overflow-x: auto;">
$$G \sim PY(G_0, \theta, d)$$
</div>

<p>のように書き，中華料理店過程と同様に予測分布は以下のようになります．</p>

<div style="overflow-x: auto;">
$$p(x_{n+1} | x_1 ... x_n) = \Sigma_{k=1}^{t} \frac{c_k - d}{c + \theta} + \frac{\theta + d_t}{c + \theta} G_0$$

$$p(x_{n+1} = w | \Theta) = \frac{c_w - d_{t_w}}{c + \theta} + \frac{\theta + d_t}{c + \theta}$$
</div>

<p>結局のところ，中華料理店過程も，Pitman-Yor過程も基底測度$G_0$に似た分布を作っているだけなのですが，中華料理店過程は$logn$のオーダーでテーブルが増えていく一方，Pitman-Yor過程は$n^d$のオーダーで増えていきます．これは冪乗則に従っており，自然言語のモデリングに適しているため，言語モデルにはPitman-Yor過程がしようされます．</p>

<h2 id="階層pitman-yor過程">階層Pitman-Yor過程</h2>

<h2 id="hpylmとスムージング">HPYLMとスムージング</h2>

<p>まずは，HPYLMの例を用いてスムージングを考えます．</p>

<p>テキストデータが以下の3文とします．</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">she will sing
she will like
he will call</code></pre></div>
<p>この時，単語列she willに続いてlikeがくる確率$P(like | she \, will)$はshe willで始まる文が2つあり，そのうち1つがshe willに続いてlikeがきているので0.5となります．</p>

<p>しかし，he willにlikeが続くデータは観測していないため，$P(like | he \, will) = 0$となります．</p>

<p>このように，観測データに存在しないものは全て確率が0となってしまうところを，0でない適切な確率を計算できるようにするのが，スムージングという手法です．</p>

<p>ここでは3-gramモデルで説明を行います．つまり，ある単語が生成される確率は，以下のように後ろの2単語のみで決まると仮定したモデルです．</p>

<div style="overflow-x: auto;">
$$ P(dog | The \, quick \, brown \, fox \, jumps \, over \, the \, lazy) = P(dog | the \, lazy)$$
</div>

<p>HPYLMでは以下のような文脈木を考え，単語のことを客，木のノードをレストランと呼びます．</p>

<p><img src="https://seiichiinoue.github.io/img/suffix_tree.png"></p>

<p>HPYLMでは，この文脈木を用いて単語の数をカウントします．</p>

<p>図の黒色の客が文脈に続いて観測された単語で，白色の客がスムージングのための代理客です．</p>

<p>例えば，she willという単語列の後にlikeという単語が何回来たかをカウントしたい場合，文脈木のルートからwill→sheとレストランを辿り，sheというレストランにlikeという客を追加します．その後，親のレストラン（ノード）に代理客を送り，再帰的に処理します．</p>

<p>これがスムージングの考え方で，HPYLMでは，中華料理店過程の一般化であるPitman-Yor過程と呼ばれる確率過程を用いて補完します．</p>

<h2 id="suffix-tree">Suffix Tree</h2>

<p>急に木構造が出てきましたが，上図はsuffix tree（接尾辞木）と言われるもので，階層的中華料理店過程のメタファーとなっているものです．</p>

<h2 id="vpylm">VPYLM</h2>

<p>VPYLMは上述の通り，HPYLMを拡張して，HPYLMでは固定で考えていたn-gramオーダーを各単語ごとに推定するようにしたモデルです．</p>

<p><img src="https://seiichiinoue.github.io/img/vpylm.png"></p>

<h2 id="学習">学習</h2>

<h2 id="実験">実験</h2>


		
	</div>

	<div class="pagination">
		<a href="https://seiichiinoue.github.io/post/cstm/" class="left arrow">&#8592;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			<span>
			&copy; <time datetime="2019-11-09 15:57:03.165162 &#43;0900 JST m=&#43;0.056830916">2019</time> Seiichi Inoue. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		</footer>

    </body>
</html>
